{
  "hash": "73c8a324ef57ef1ccba309cb6a309bf3",
  "result": {
    "markdown": "---\ntitle: \"Beginning Predictions\"\ndate: last-modified\neditor_options: \n  markdown: \n    wrap: sentence\n---\n\nIn this meeting, we'll train our first model and see how to gather metrics and predictions.\nFirst, let's go over the homework from last time.\n\n# Homework from last time\n\nThere were three \"homework questions\" from last time:\n\n-   Repeat the EDA and visualizations for the 2019 data set.\n-   Commit and push your changes\n-   Combine the 2019 and 2020 datasets.\n\nThe first two are pretty straightforward, so I'll cover how to combine the 2018-2021 datasets.\nIt's important that we combine the datasets correctly as this dataset is what we'll be using going forward in these meetings.\n\n## Combining the datasets\n\nBefore we get started, let's check out the other spreadsheets.\nWe already saw what was in the 2018 data, but let's open the Excel spreadsheets for the other datasets to see how straightforward combining the datasets will be.\n\nAfter opening up the datasets, here are the primary issues facing us:\n\n-   *Columns have different names between the different datasets.* For example, the Microcystin measurement is labeled \"Microcystin RAW Value \\[ug/L\\]\" in the 2018 data set but is called \"Microcystin\" in all of the other data sets. Another example is that the gene copy numbers of the various *mcy*A species (*mcy*A P/M/A) are labeled a few different ways.\n-   *Not all columns are shared between datasets.* This is most obvious when comparing the 2021 dataset versus the other years.\n\nWith these in mind, our plan of attack will be, for each data set:\n\n1.  Select only those columns that appear in the 2021 dataset\n2.  Convert column names to a standard set of names\n3.  Add a year column\n\nAfter we do all of these steps, we'll concatenate all of the datasets together.\n\nNote that this is just one way to handle this problem.\nAnother solution might be to use *all* of the columns for all years and impute the missing data for the missing columns, or to select a learning algorithm that can handle the missing data.\nHowever, doing so introduces more uncertainty into the model we'll use down the line, and so this particular method was chosen because...\n\n### 2021 Data\n\nAnyway, let's start by reading in the 2021 data since we'll only be using columns found for that year.\nAfter reading in the data, we'll call `clean_names` and filter out any samples without a `collected_date`.\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\ndnr.2021 <- read_xlsx(\"data/IowaDNR_2021_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2021 %>% \n  colnames()\n```\n\n::: {.cell-output-stdout}\n```\n [1] \"label\"                  \"collected_date\"         \"environmental_location\"\n [4] \"microcystin\"            \"tkp\"                    \"ortho_p\"               \n [7] \"tkn\"                    \"p_h\"                    \"dissolved_oxygen_mg_l\" \n[10] \"x16s\"                   \"am\"                     \"ap\"                    \n[13] \"aa\"                    \n```\n:::\n:::\n\nMost of these columns look fine, but let's change the *mcy* variable names from `ax` to `mcy_a_x` to be more descriptive (with the added benefit of matching how `clean_names` parses these columns in the other datasets).\nThere are a couple ways we can do this, but I'll be using the rename function here:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2021 <- dnr.2021 %>% \n  rename(\n    mcy_a_m = am,\n    mcy_a_p = ap,\n    mcy_a_a = aa\n  )\n\ndnr.2021 %>% \n  colnames()\n```\n\n::: {.cell-output-stdout}\n```\n [1] \"label\"                  \"collected_date\"         \"environmental_location\"\n [4] \"microcystin\"            \"tkp\"                    \"ortho_p\"               \n [7] \"tkn\"                    \"p_h\"                    \"dissolved_oxygen_mg_l\" \n[10] \"x16s\"                   \"mcy_a_m\"                \"mcy_a_p\"               \n[13] \"mcy_a_a\"               \n```\n:::\n:::\n\nFinally, we need to take care of is type of `pH`. Because there are row values of `No Data` in the spreadsheet it comes from, the `p_h` column gets read in as a character variable.\nLet's mutate this column to using `as.numeric`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2021 <- dnr.2021 %>% \n  mutate(p_h = as.numeric(p_h)) \n```\n\n::: {.cell-output-stderr}\n```\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n```\n:::\n:::\n\nGreat!\nWe now know what we want the names of our columns to be and we're ready to repeat this process for the other data sets.\nLet's revisit the 2018 data.\n\n### 2018 Data\n\nLet's start by looking at our column names:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2018 <- read_xlsx(\"data/IowaDNR_2018_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2018 %>% \n  colnames()\n```\n\n::: {.cell-output-stdout}\n```\n [1] \"sample_id\"                         \"environmental_location\"           \n [3] \"collected_date\"                    \"microcystin_raw_value_ug_l\"       \n [5] \"p_h\"                               \"doc_ppm\"                          \n [7] \"tkp_mg_p_l\"                        \"tkn_mg_n_l\"                       \n [9] \"nh3_mg_n_l\"                        \"n_ox_mg_n_l\"                      \n[11] \"no2_mg_n_l\"                        \"cl_mg_cl_l\"                       \n[13] \"so4_mg_so4_l\"                      \"x16s_r_rna_gene_copies_m_l\"       \n[15] \"microcystismcy_a_gene_copies_m_l\"  \"aanabaenamcy_a_gene_copies_m_l\"   \n[17] \"planktothrixmcy_a_gene_copies_m_l\"\n```\n:::\n:::\n\nThe ideal situation is that all the column names match exactly and we can just use `any_of` to get those columns from this data frame.\nHowever, when we try that, we see that we still have some work to do:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2018 %>% \n  select(any_of(colnames(dnr.2021)))\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 539 × 3\n   collected_date      environmental_location   p_h\n   <dttm>              <chr>                  <dbl>\n 1 2018-05-22 11:19:00 Backbone Beach           8.1\n 2 2018-06-19 09:00:00 Backbone Beach           8.8\n 3 2018-08-21 13:00:00 Backbone Beach           8.9\n 4 2018-07-10 11:15:00 Backbone Beach           8.2\n 5 2018-08-28 12:30:00 Backbone Beach           8.8\n 6 2018-08-14 10:30:00 Backbone Beach           8  \n 7 2018-08-07 11:10:00 Backbone Beach           7.7\n 8 2018-07-24 12:00:00 Backbone Beach           8.9\n 9 2018-07-31 11:25:00 Backbone Beach           8.5\n10 2018-07-02 11:15:00 Backbone Beach           7.9\n# … with 529 more rows\n```\n:::\n:::\n\nWe're missing a lot of columns here.\nLet's do another `rename` step here before our select step:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2018 <- dnr.2018 %>% \n  rename(\n    microcystin = microcystin_raw_value_ug_l,\n    tkp = tkp_mg_p_l,\n    tkn = tkn_mg_n_l,\n    x16s = x16s_r_rna_gene_copies_m_l,\n    mcy_a_m = microcystismcy_a_gene_copies_m_l,\n    mcy_a_p = planktothrixmcy_a_gene_copies_m_l,\n    mcy_a_a = aanabaenamcy_a_gene_copies_m_l\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n\ndnr.2018 %>% \n  head()\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 × 10\n  collected_date      environmental_locati… microcystin   tkp   tkn   p_h   x16s\n  <dttm>              <chr>                       <dbl> <dbl> <dbl> <dbl>  <dbl>\n1 2018-05-22 11:19:00 Backbone Beach              0.63   1.61 0.66    8.1 1.47e5\n2 2018-06-19 09:00:00 Backbone Beach              0.46   1.06 0.517   8.8 4.23e2\n3 2018-08-21 13:00:00 Backbone Beach              0.438  1.06 0.169   8.9 4.04e7\n4 2018-07-10 11:15:00 Backbone Beach              0.425  1.22 0.505   8.2 1.08e7\n5 2018-08-28 12:30:00 Backbone Beach              0.418  1.02 1.14    8.8 7.65e6\n6 2018-08-14 10:30:00 Backbone Beach              0.405  1.15 0.298   8   4.46e7\n# … with 3 more variables: mcy_a_m <dbl>, mcy_a_p <dbl>, mcy_a_a <dbl>\n```\n:::\n:::\n\nMuch better.\nNote that there are some missing columns here because there are columns in the 2021 dataset that don't exist in the 2018 dataset.\n\n### 2019 Data\n\nWe'll repeat this process for the 2019 dataset.\nSome notes:\n\n-   `clean_names` already nicely formatted the *mcy*X columns\n-   By default, `read_xlsx` reads in the first sheet of the `xlsx` file. This means that the 2019 data will only show the data on the first sheet. We want the data from the sheet labeled \"combined\", so use the `sheet` argument of `read_xlsx` to do this.\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2019 <- read_xlsx(\"data/IowaDNR_2019_Data_Merged.xlsx\",\n                      sheet = \"combined\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date)) %>% \n  rename(\n    tkn = tkn_mg_n_l,\n    tkp = tkp_mg_p_l,\n    ortho_p = ortho_p_mg_p_l\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2019 %>% \n  head()\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 × 12\n  label    collected_date      environmental_l… microcystin    tkp ortho_p   tkn\n  <chr>    <dttm>              <chr>                  <dbl>  <dbl>   <dbl> <dbl>\n1 6-21280… 2019-06-25 00:00:00 Backbone Beach         0      0.473  0.065  0.18 \n2 7-21280… 2019-07-01 00:00:00 Backbone Beach         0      0.023  0.034  0.282\n3 8-21280… 2019-07-09 00:00:00 Backbone Beach         0.23   0.056  0.0401 0.346\n4 9-21280… 2019-07-16 00:00:00 Backbone Beach         0.743 -0.005  0.023  0.365\n5 10-2128… 2019-07-23 00:00:00 Backbone Beach         0.038  0.287  0.081  0.858\n6 11-2128… 2019-07-30 00:00:00 Backbone Beach         0.105  0.199  0.22   1.18 \n# … with 5 more variables: p_h <dbl>, x16s <dbl>, mcy_a_m <dbl>, mcy_a_p <dbl>,\n#   mcy_a_a <dbl>\n```\n:::\n:::\n\n### 2020 Data\n\nPretty much the same process as above, though we need to cast the `ortho_p`, `tkn`, and `tkp` columns as a numeric to account for places where the text value \"NA\" is entered:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.2020 <- read_xlsx(\"data/IowaDNR_2020_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date)) %>% \n  rename(\n    tkp = tkp_mg_p_l,\n    tkn = tkn_mg_n_l, \n    ortho_p = ortho_p_mg_p_l\n  ) %>% \n  mutate(\n    across(c(ortho_p, tkn, tkp), ~ as.numeric(.))\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-stderr}\n```\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n```\n:::\n:::\n\n### Putting it all together\n\nSince we did all of the cleaning ahead of time, combining the columns now will be straightforward using `bind_rows`.\nLet's save this to a new variable called `dnr_all`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.all <- bind_rows(\n  dnr.2018,\n  dnr.2019,\n  dnr.2020,\n  dnr.2021\n)\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Should we have written functions?\n\nNotice that there were a lot of redundant steps across all of the DNR sets.\nFor each data set, we:\n\n1.  Read in the Excel spreadsheet\n2.  Called `clean_names`\n3.  Filtered `na` dates\n4.  Renamed some columns\n5.  Selected the columns that match the names of the 2021 dataset.\n\nIdeally, we would have written a function to handle all of the redundant tasks.\nFor example, we could have written a function `read_dnr_sheet` that takes in a file path, reads in the Excel sheet, calls `clean_names`, filters `na` dates, renames columns, and selects only those columns that match those from the 2021 data.\n\nSo yes, it would have been nice to write functions.\nHowever, the problem is that the data sheets are all just different enough that this isn't exactly quite worth the effort.\nFor instance, the 2019 data requires the `sheet` argument, which complicates the potential function definition.\nSimilarly, the column names that are required to be renamed don't quite match up between the different years, and so would require an additional argument.\n:::\n\n# Some brief EDA\n\nBefore we jump into predictions, let's do some quick visualizations.\nFirst, let's create the `hazard_class` column to record whether or not the microcystin concentration was over 8 ug/L for that observation, as well as a `year` column so we can separate the samples by the year they were collected in.\nFinally, we'll remove observations that don't have a valid hazard class:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\n\ndnr.all <- dnr.all %>% \n  mutate(\n    hazard_class = if_else(microcystin > 8, \"hazardous\",\"safe\"),\n    year = year(collected_date),\n    dissolved_oxygen_mg_l = as.numeric(dissolved_oxygen_mg_l)\n    ) %>% \n  filter(!is.na(hazard_class))\n```\n:::\n\n## How many hazardous cases were there per year?\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_light())\n\ndnr.all %>% \n  filter(hazard_class == \"hazardous\") %>% \n  count(year) %>% \n  ggplot(aes(year, n, fill = n)) +\n  geom_col() + \n  scale_fill_viridis_c(option = \"inferno\", direction = -1, end = 0.5) + \n  labs(\n    x = \"Sample year\",\n    y = \"# HABs Observed\",\n    title = \"# Harmful algal blooms observed per year\",\n    fill = \"# HABs\\nobserved\"\n  ) + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.border = element_blank(),\n    axis.line.x = element_line(color = \"black\", size = 0.5),\n    axis.line.y = element_line(color = \"black\", size = 0.5),\n    axis.ticks = element_blank()\n  ) + \n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1), add = 0)\n  ) \n```\n\n::: {.cell-output-display}\n![](first_model_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n## Which locations had the most harmful algal blooms?\n\n::: {.cell}\n\n```{.r .cell-code}\ndnr.all %>% \n  filter(hazard_class == \"hazardous\") %>% \n  count(environmental_location) %>%\n  mutate(environmental_location = fct_reorder(environmental_location, n)) %>% \n  ggplot(aes(n, environmental_location, fill = n)) + \n  geom_col() + \n  labs(\n    x = \"# HABS observed (all years)\",\n    y = \"Sample Location\",\n    title = \"Total number of HABs observed for all years\",\n    fill = \"# HABs\\nObserved\"\n  ) + \n  scale_fill_viridis_c(option = \"magma\", direction = -1) + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.ticks = element_blank()\n  ) + \n  scale_x_continuous(\n    expand = expansion(mult = c(0, 0.1), add = 0)\n  ) \n```\n\n::: {.cell-output-display}\n![](first_model_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n# Intro to Tidymodels\n\nUp until now, we've just been doing EDA stuff.\nNow that we've done some of that, we can get started with modeling.\nFor this series, we're going to use `tidymodels` to keep our prediction workflows organized and reproducible.\n\nThe [`tidymodels`](https://www.tidymodels.org/) framework is \"a collection of packages for modeling and machine learning using tidyverse principles.\" The `tidymodels` website has [several tutorials](https://www.tidymodels.org/start/) to introduce you to the fundamentals of working with `tidymodels`.\nToday, we're going to use the `tidymodels` framework to:\n\n1. Split our data\n1. Perform some preprocessing\n1. Train a linear model\n1. Visualize some metrics\n\n## Splitting the data, preprocessing\n\nWe begin by splitting our data into testing and training splits:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(489)\n\ndata_split <- initial_split(dnr.all, strata = \"hazard_class\")\n\ntraining_data <- training(data_split)\ntesting_data <- testing(data_split)\n```\n:::\n\nNext, we create a recipe. \nA recipe is a series of data processing steps to perform on our data before we feed it into our model. \nHere, the only steps we'll take is to remove non-informative variables and to impute some missing values by filling in the mean.\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_recipe <- recipe(microcystin ~ ., training_data) %>% \n  step_rm(collected_date, label) %>% \n  step_impute_mean(all_numeric())\n```\n:::\n\nWe can take a look at the preprocessed data set by using the `prep` and `juice` functions. \n`prep` will perform the steps outlined in the pre-processing recipe. \nFor example, in our recipe above, it will calculate the mean of all the numeric columns. \n`juice` will then return the transformed dataset resulting from following all the steps in the recipe:\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_recipe %>% \n  prep %>% \n  juice\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1,193 × 13\n   environmental_loca…   tkp    tkn   p_h   x16s mcy_a_m mcy_a_p mcy_a_a ortho_p\n   <fct>               <dbl>  <dbl> <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Backbone Beach      1.61   0.66   8.1  1.47e5       0       0       0  0.0430\n 2 Backbone Beach      1.06   0.517  8.8  4.23e2       0       0       0  0.0430\n 3 Backbone Beach      1.22   0.505  8.2  1.08e7    1240       0       0  0.0430\n 4 Backbone Beach      1.02   1.14   8.8  7.65e6       0       0       0  0.0430\n 5 Backbone Beach      1.15   0.298  8    4.46e7       0       0       0  0.0430\n 6 Backbone Beach      0.922 -0.24   7.7  2.66e7       0       0       0  0.0430\n 7 Backbone Beach      0.041  0.233  8.9  5.04e6       0       0       0  0.0430\n 8 Backbone Beach      0.491  0.264  8.5  1.09e7     432       0       0  0.0430\n 9 Backbone Beach      1.12   0.437  7.9  6.09e6       0       0       0  0.0430\n10 Backbone Beach      1.22   0.569  7.28 4.44e6       0       0       0  0.0430\n# … with 1,183 more rows, and 4 more variables: dissolved_oxygen_mg_l <dbl>,\n#   hazard_class <fct>, year <dbl>, microcystin <dbl>\n```\n:::\n:::\n\nWe see that the `collected_date` and `label` were removed, and the numeric columns had the missing data filled in with the column mean.\n\n## Creating and training a model\n\nNow that we have our data preprocessing steps, we'll create the model that we want to use to predict.\nTo begin with, we'll just use an `lm` model.\nWe'll indicate this to `tidymodels` using the `linear_reg` function.\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- linear_reg() %>% \n  set_engine(\"lm\") %>% \n  set_mode(\"regression\")\nlinear_model\n```\n\n::: {.cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\nThis tells us that `linear_model` is a `tidymodels` object using the `lm` function to perform linear regression.\n\nNow we can package the preprocessing recipe and the model into a workflow. \nThink of the workflow as a pipeline - a series of steps you want to carry out on the data.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_workflow <- workflow() %>% \n  add_recipe(hab_recipe) %>% \n  add_model(linear_model) \nlm_workflow\n```\n\n::: {.cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_impute_mean()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\nThis shows us all of the preprocessing steps we're going to carry out on the data as well as the model we're going to use.\nWe're now ready to fit our model to the training data:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm_workflow %>% \n  last_fit(split = data_split)\n```\n\n::: {.cell-output-stderr}\n```\n! train/test split: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici...\n```\n:::\n\n```{.r .cell-code}\nlm_fit\n```\n\n::: {.cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [1193/398]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n\nUse `collect_notes(object)` for more information.\n```\n:::\n:::\n\nThe `last_fit` function takes the entire data split, performs the preprocessing and model training steps on the training set, then predicts on the testing set.\nThere are several convenience functions we can use to interact with the `lm_fit` object. \nTo start, we can collect the metrics via `collect_metrics`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% \n  collect_metrics\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       3.42  Preprocessor1_Model1\n2 rsq     standard       0.692 Preprocessor1_Model1\n```\n:::\n:::\n\nWe can also collect the predictions via `collect_predictions`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% \n  collect_predictions() \n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 398 × 5\n   id               .pred  .row microcystin .config             \n   <chr>            <dbl> <int>       <dbl> <chr>               \n 1 train/test split 0.967     3       0.438 Preprocessor1_Model1\n 2 train/test split 0.661    13       0.153 Preprocessor1_Model1\n 3 train/test split 1.53     21       0.377 Preprocessor1_Model1\n 4 train/test split 2.12     23       0.195 Preprocessor1_Model1\n 5 train/test split 2.03     30       0     Preprocessor1_Model1\n 6 train/test split 0.654    31       0.855 Preprocessor1_Model1\n 7 train/test split 0.837    34       0.69  Preprocessor1_Model1\n 8 train/test split 0.677    36       0.405 Preprocessor1_Model1\n 9 train/test split 0.673    43       0.003 Preprocessor1_Model1\n10 train/test split 0.423    44       0     Preprocessor1_Model1\n# … with 388 more rows\n```\n:::\n:::\n\nThis is perhaps more useful when visualized:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% \n  collect_predictions() %>% \n  ggplot(aes(.pred, microcystin)) + \n  geom_point(color = \"#0000FF\", alpha = 0.5) + \n  geom_abline(color = \"red\", linetype = 2, alpha = 0.5) + \n  theme(\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Linear Regression Prediction Results - dnr.all dataset\"\n  ) \n```\n\n::: {.cell-output-display}\n![](first_model_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n::: {.callout-note collapse=\"true\"}\n\nTypically when working with `tidymodels`, there are a few intermediate steps before jumping straight to `last_fit` and looking at predictions.\nFor another illustration of using `tidymodels` on the built-in `diamonds` data set, see the code below, borrowed from [this StackOverflow post](https://stackoverflow.com/a/63239991/6024276):\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(489)\ndiamonds_split <- initial_split(diamonds, prop = 4/5)\n\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\ndiamonds_recipe <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_log(all_outcomes(),skip = T) %>%\n  step_normalize(all_predictors(), -all_nominal()) %>%\n  step_dummy(all_nominal()) %>%\n  step_poly(carat, degree = 2)\n\n\nlr_model <- \n  linear_reg()%>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nlr_workflow <- workflow() %>%\n  add_model(lr_model) %>% \n  add_recipe(diamonds_recipe) \n\nfinal_model <- fit(lr_workflow, diamonds)\n\npredict(final_model, new_data = as.data.frame(diamonds_test))\n```\n:::\n\n:::\n\n\n\n\n# Homework\n\n-   So far, we've been doing all of our work within one `qmd`. However, the code to read in the data and combine it is getting quite big. Let's simplify this document by moving our data prep (reading in the data sets, cleaning the data, combining the data) into a R script file called `load_DNR_data.R`. After creating `load_DNR_data.R`, `source` the script at the top of your `qmd`.\n-   Recreate the plot below:\n\n::: {.cell}\n::: {.cell-output-display}\n![](first_model_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\nHere are some hints:\n\n\n::: {.callout-note collapse=\"true\"}\n## Filling in the missing values\n\nThere are a couple different ways to fill in the missing values. \nThe way I achived this was by using `pivot_wider`, `replace`, and `pivot_longer`.\nAnother potential way is to use `join/anti_join`.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## The fonts\n\nThe font used in this visualization is [Patua One](https://fonts.google.com/specimen/Patua+One). \nI used the `showtext` and `ggtext` packages in order to use the font.\nSpecifically, the `font_add_google`, `showtext_auto`, and `element_textbox` functions were used.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## The highlights in the title\n\nThe `glue` package was used to construct the title. \nThe words to be highlighted were surrounded by `span` elements to change the color. \nThe color used was `#0000FF`. \n\n:::\n\n* Evaluate the performance of the linear regression on the prediction of HAB occurrence.\n* There was a warning when we created the `lm_fit` object. What was the warning? What does that mean?\n    * Hint: Check out `lm_fit$.workflow[[1]] %>% tidy()`\n* What are some problems you can see with the current state of `dnr.all`?\n* What are some issues you see with the correct predictions output by `lm_fit`?\n* Add another preprocessing step to the model recipe. For example, add a step to normalize the data or another step to turn `environmental_location` into a dummy variable.\n* Install the `xgboost` library and change the model used to `xgboost`. Between classification and regression, which models performed better? Why? \n    * Hint: Use `boost_tree` and `set_mode(\"classification\")`\n* Install the `glmnet` library and use a logistic regression model. Between the XGBoost and logistic regression models, which performed better?\n* Our data is super imbalanced. How should we handle this?\n* Look at summary of `lm_fit` again. What red flags are you seeing?\n* What additional steps should we be doing in our model training workflow?",
    "supporting": [
      "first_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}