[
  {
    "objectID": "docs/presentations/testpres.html#getting-up",
    "href": "docs/presentations/testpres.html#getting-up",
    "title": "Test",
    "section": "Getting up",
    "text": "Getting up\nAnd doing stuff"
  },
  {
    "objectID": "docs/presentations/testpres.html#coming-down",
    "href": "docs/presentations/testpres.html#coming-down",
    "title": "Test",
    "section": "Coming down",
    "text": "Coming down\nAnd finishing"
  },
  {
    "objectID": "docs/meetings/getting_started.html",
    "href": "docs/meetings/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "For the first lesson, we’ll set up the project we’ll work in for the next several meetings. By the end of today, we’ll have:\nAlong the way, we’ll be using Quarto documents (successor to RMarkdown documents) so that our work is well-documented and ready to share. Before we get started with all of that, though, let’s familiarize ourselves with the data we’ll be working with."
  },
  {
    "objectID": "docs/meetings/getting_started.html#the-iowa-dnrs-lake-monitoring-dataset",
    "href": "docs/meetings/getting_started.html#the-iowa-dnrs-lake-monitoring-dataset",
    "title": "Getting Started",
    "section": "The Iowa DNR’s Lake Monitoring Dataset",
    "text": "The Iowa DNR’s Lake Monitoring Dataset\nFor the past several decades, the Iowa Department of Natural Resources has been monitoring the water quality of multiple beaches throughout Iowa in order to better understand harmful algal blooms. Between Memorial and Labor day, weekly water samples are taken from these beaches and several chemical and biological measurements are taken. In addition to these measurements, we will incorporate land-use classifications and weather data.\n\n\n\n\n\nThe beaches that the Iowa DNR collected water samples from in 2018.\n\n\nThe measurement of interest is the microcystin concentration, measured in ug/L. When this measurement is above the EPA-recommend 8 ug/L, the beach is closed as it is considered dangerous to humans at that point. Hence, this is considered a supervised learning problem.\nThe goal of the project will be to use this data to:\n\nAttempt to better understand what drives harmful algal blooms\nCreate a model that can predict harmful algal blooms\n\nNow that we know a little bit about the data we’ll be working with and the problem we’re being asked to solve, let’s go ahead and set up our project."
  },
  {
    "objectID": "docs/meetings/getting_started.html#checking-year-counts",
    "href": "docs/meetings/getting_started.html#checking-year-counts",
    "title": "Getting Started",
    "section": "Checking year counts",
    "text": "Checking year counts\nLet’s start off by counting the samples by year to make sure that the dataset doesn’t have any observations from other years. Since collected_date is a datetime object (as denoted by the dttm beneath the column name), we can use the year function from lubridate to count:\n\ndnr.2018 %>% \n  count(year(collected_date))\n\n# A tibble: 2 × 2\n  `year(collected_date)`     n\n                   <dbl> <int>\n1                   2018   539\n2                     NA     1\n\n\nIt looks like there’s an observation that doesn’t have the year for some reason. Let’s take a look at it:\n\ndnr.2018 %>% \n  filter(is.na(collected_date))\n\n# A tibble: 1 × 17\n  sample_id  environmental_l… collected_date microcystin_raw_valu…   p_h doc_ppm\n  <chr>      <chr>            <dttm>                         <dbl> <dbl>   <dbl>\n1 4-21070001 George Wyth Bea… NA                                NA    NA      NA\n# … with 11 more variables: tkp_mg_p_l <dbl>, tkn_mg_n_l <dbl>,\n#   nh3_mg_n_l <chr>, n_ox_mg_n_l <dbl>, no2_mg_n_l <chr>, cl_mg_cl_l <chr>,\n#   so4_mg_so4_l <chr>, x16s_r_rna_gene_copies_m_l <dbl>,\n#   microcystismcy_a_gene_copies_m_l <dbl>,\n#   aanabaenamcy_a_gene_copies_m_l <dbl>,\n#   planktothrixmcy_a_gene_copies_m_l <dbl>\n\n\nFor some reason, this observation has no data, but has a sample_id and location. Checking the original Excel file confirms that this is the case, and so we can remove this row as an additional step when we read in the file. The updated command to read in the file is:\n\ndnr.2018 <- read_xlsx(\"data/IowaDNR_2018_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date))\ndnr.2018 %>% \n  head()\n\n\n\n# A tibble: 6 × 17\n  sample_id  environmental_l… collected_date      microcystin_raw…   p_h doc_ppm\n  <chr>      <chr>            <dttm>                         <dbl> <dbl>   <dbl>\n1 1-21280001 Backbone Beach   2018-05-22 11:19:00            0.63    8.1    5.85\n2 5-21280001 Backbone Beach   2018-06-19 09:00:00            0.46    8.8    1.45\n3 14-212800… Backbone Beach   2018-08-21 13:00:00            0.438   8.9    2.15\n4 8-21280001 Backbone Beach   2018-07-10 11:15:00            0.425   8.2    1.91\n5 15-212800… Backbone Beach   2018-08-28 12:30:00            0.418   8.8    1.73\n6 13-212800… Backbone Beach   2018-08-14 10:30:00            0.405   8      1.50\n# … with 11 more variables: tkp_mg_p_l <dbl>, tkn_mg_n_l <dbl>,\n#   nh3_mg_n_l <chr>, n_ox_mg_n_l <dbl>, no2_mg_n_l <chr>, cl_mg_cl_l <chr>,\n#   so4_mg_so4_l <chr>, x16s_r_rna_gene_copies_m_l <dbl>,\n#   microcystismcy_a_gene_copies_m_l <dbl>,\n#   aanabaenamcy_a_gene_copies_m_l <dbl>,\n#   planktothrixmcy_a_gene_copies_m_l <dbl>\n\n\nLet’s count again to confirm this works:\n\ndnr.2018 %>% \n  count(year(collected_date))\n\n# A tibble: 1 × 2\n  `year(collected_date)`     n\n                   <dbl> <int>\n1                   2018   539\n\n\nGreat! Now all of the remaining data has the correct year."
  },
  {
    "objectID": "docs/meetings/getting_started.html#exploring-microcystin",
    "href": "docs/meetings/getting_started.html#exploring-microcystin",
    "title": "Getting Started",
    "section": "Exploring microcystin",
    "text": "Exploring microcystin\nSince the microcystin concentration is how we’re going to label the samples, we need to check that they’re all there. Similar to the above, we can count the number of missing rows using is.na and count:\n\ndnr.2018 %>% \n  count(is.na(microcystin_raw_value_ug_l))\n\n# A tibble: 1 × 2\n  `is.na(microcystin_raw_value_ug_l)`     n\n  <lgl>                               <int>\n1 FALSE                                 539\n\n\nThis tells us that there are no missing values in the microcystin column, which is good since we won’t need to throw any data away.\nNow let’s see how the microcystin is distributed:\n\ndnr.2018 %>% \n  ggplot(aes(microcystin_raw_value_ug_l)) +\n  geom_histogram(color = \"black\", fill = \"white\") +\n  labs(\n    x = \"Microcystin (ug/L)\",\n    y = \"Count\",\n    title = \"Microcystin concentration (all lakes, all weeks)\"\n  ) + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\nThe data shows a long right tail. This is our first hint that this is a highly imbalanced dataset.\n\nCreating the hazard_class variable\nTo get a better idea of this, let’s create a new class variable based on the microcystin concentration: if the microcystin is greater than 8 ug/L, we’ll say that it is hazardous, and safe otherwise. For now, let’s just check print out the results and check that it works as we expected it to:\n\ndnr.2018 %>% \n  mutate(hazard_class = if_else(\n    microcystin_raw_value_ug_l > 8,\n    \"hazardous\",\n    \"safe\"\n  )) %>% \n  select(microcystin_raw_value_ug_l, hazard_class)\n\n# A tibble: 539 × 2\n   microcystin_raw_value_ug_l hazard_class\n                        <dbl> <chr>       \n 1                      0.63  safe        \n 2                      0.46  safe        \n 3                      0.438 safe        \n 4                      0.425 safe        \n 5                      0.418 safe        \n 6                      0.405 safe        \n 7                      0.39  safe        \n 8                      0.3   safe        \n 9                      0.237 safe        \n10                      0.205 safe        \n# … with 529 more rows\n\n\nOkay, this looks correct. Let’s update our code that reads in the data to include this before we forget:\n\ndnr.2018 <- read_xlsx(\"data/IowaDNR_2018_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date)) %>% \n  mutate(hazard_class = if_else(\n    microcystin_raw_value_ug_l > 8,\n    \"hazardous\",\n    \"safe\"\n    ))\ndnr.2018 %>% \n  head()\n\n\n\n# A tibble: 6 × 18\n  sample_id  environmental_l… collected_date      microcystin_raw…   p_h doc_ppm\n  <chr>      <chr>            <dttm>                         <dbl> <dbl>   <dbl>\n1 1-21280001 Backbone Beach   2018-05-22 11:19:00            0.63    8.1    5.85\n2 5-21280001 Backbone Beach   2018-06-19 09:00:00            0.46    8.8    1.45\n3 14-212800… Backbone Beach   2018-08-21 13:00:00            0.438   8.9    2.15\n4 8-21280001 Backbone Beach   2018-07-10 11:15:00            0.425   8.2    1.91\n5 15-212800… Backbone Beach   2018-08-28 12:30:00            0.418   8.8    1.73\n6 13-212800… Backbone Beach   2018-08-14 10:30:00            0.405   8      1.50\n# … with 12 more variables: tkp_mg_p_l <dbl>, tkn_mg_n_l <dbl>,\n#   nh3_mg_n_l <chr>, n_ox_mg_n_l <dbl>, no2_mg_n_l <chr>, cl_mg_cl_l <chr>,\n#   so4_mg_so4_l <chr>, x16s_r_rna_gene_copies_m_l <dbl>,\n#   microcystismcy_a_gene_copies_m_l <dbl>,\n#   aanabaenamcy_a_gene_copies_m_l <dbl>,\n#   planktothrixmcy_a_gene_copies_m_l <dbl>, hazard_class <chr>\n\n\nAnd now, let’s see just how imbalanced these samples are:\n\ndnr.2018 %>% \n  count(hazard_class) %>% \n  mutate()\n\n# A tibble: 2 × 2\n  hazard_class     n\n  <chr>        <int>\n1 hazardous       17\n2 safe           522\n\n\nSo, of the 539 total samples, only 17 of them are considered hazardous. To put numbers on how imbalanced this is:\n\ndnr.2018 %>% \n  group_by(hazard_class) %>% \n  summarise(n = n()) %>% \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  hazard_class     n   prop\n  <chr>        <int>  <dbl>\n1 hazardous       17 0.0315\n2 safe           522 0.968 \n\n\nOnly 3% of our samples belong to the minority class. This is an extremely imbalanced dataset and we must address this when we do our model training and predictions.\n\n\n\n\n\n\nNote that putting the microcystin on a log scale makes the distribution appear normal. This is an example of a log-normal distribution. Here is some further reading:\n\nA good post about its statistical properties\nA post about making predictions on a log-normal distribution"
  },
  {
    "objectID": "docs/meetings/first_model.html",
    "href": "docs/meetings/first_model.html",
    "title": "Beginning Predictions",
    "section": "",
    "text": "In this meeting, we’ll train our first model and see how to gather metrics and predictions. First, let’s go over the homework from last time."
  },
  {
    "objectID": "docs/meetings/first_model.html#combining-the-datasets",
    "href": "docs/meetings/first_model.html#combining-the-datasets",
    "title": "Beginning Predictions",
    "section": "Combining the datasets",
    "text": "Combining the datasets\nBefore we get started, let’s check out the other spreadsheets. We already saw what was in the 2018 data, but let’s open the Excel spreadsheets for the other datasets to see how straightforward combining the datasets will be.\nAfter opening up the datasets, here are the primary issues facing us:\n\nColumns have different names between the different datasets. For example, the Microcystin measurement is labeled “Microcystin RAW Value [ug/L]” in the 2018 data set but is called “Microcystin” in all of the other data sets. Another example is that the gene copy numbers of the various mcyA species (mcyA P/M/A) are labeled a few different ways.\nNot all columns are shared between datasets. This is most obvious when comparing the 2021 dataset versus the other years.\n\nWith these in mind, our plan of attack will be, for each data set:\n\nSelect only those columns that appear in the 2021 dataset\nConvert column names to a standard set of names\nAdd a year column\n\nAfter we do all of these steps, we’ll concatenate all of the datasets together.\nNote that this is just one way to handle this problem. Another solution might be to use all of the columns for all years and impute the missing data for the missing columns, or to select a learning algorithm that can handle the missing data. However, doing so introduces more uncertainty into the model we’ll use down the line, and so this particular method was chosen because…\n\n2021 Data\nAnyway, let’s start by reading in the 2021 data since we’ll only be using columns found for that year. After reading in the data, we’ll call clean_names and filter out any samples without a collected_date.\n\n\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\ndnr.2021 <- read_xlsx(\"data/IowaDNR_2021_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date))\n\n\ndnr.2021 %>% \n  colnames()\n\n [1] \"label\"                  \"collected_date\"         \"environmental_location\"\n [4] \"microcystin\"            \"tkp\"                    \"ortho_p\"               \n [7] \"tkn\"                    \"p_h\"                    \"dissolved_oxygen_mg_l\" \n[10] \"x16s\"                   \"am\"                     \"ap\"                    \n[13] \"aa\"                    \n\n\nMost of these columns look fine, but let’s change the mcy variable names from ax to mcy_a_x to be more descriptive (with the added benefit of matching how clean_names parses these columns in the other datasets). There are a couple ways we can do this, but I’ll be using the rename function here:\n\ndnr.2021 <- dnr.2021 %>% \n  rename(\n    mcy_a_m = am,\n    mcy_a_p = ap,\n    mcy_a_a = aa\n  )\n\ndnr.2021 %>% \n  colnames()\n\n [1] \"label\"                  \"collected_date\"         \"environmental_location\"\n [4] \"microcystin\"            \"tkp\"                    \"ortho_p\"               \n [7] \"tkn\"                    \"p_h\"                    \"dissolved_oxygen_mg_l\" \n[10] \"x16s\"                   \"mcy_a_m\"                \"mcy_a_p\"               \n[13] \"mcy_a_a\"               \n\n\nFinally, we need to take care of is type of pH. Because there are row values of No Data in the spreadsheet it comes from, the p_h column gets read in as a character variable. Let’s mutate this column to using as.numeric:\n\ndnr.2021 <- dnr.2021 %>% \n  mutate(p_h = as.numeric(p_h)) \n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\nGreat! We now know what we want the names of our columns to be and we’re ready to repeat this process for the other data sets. Let’s revisit the 2018 data.\n\n\n2018 Data\nLet’s start by looking at our column names:\n\ndnr.2018 <- read_xlsx(\"data/IowaDNR_2018_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date))\n\n\n\n\n\ndnr.2018 %>% \n  colnames()\n\n [1] \"sample_id\"                         \"environmental_location\"           \n [3] \"collected_date\"                    \"microcystin_raw_value_ug_l\"       \n [5] \"p_h\"                               \"doc_ppm\"                          \n [7] \"tkp_mg_p_l\"                        \"tkn_mg_n_l\"                       \n [9] \"nh3_mg_n_l\"                        \"n_ox_mg_n_l\"                      \n[11] \"no2_mg_n_l\"                        \"cl_mg_cl_l\"                       \n[13] \"so4_mg_so4_l\"                      \"x16s_r_rna_gene_copies_m_l\"       \n[15] \"microcystismcy_a_gene_copies_m_l\"  \"aanabaenamcy_a_gene_copies_m_l\"   \n[17] \"planktothrixmcy_a_gene_copies_m_l\"\n\n\nThe ideal situation is that all the column names match exactly and we can just use any_of to get those columns from this data frame. However, when we try that, we see that we still have some work to do:\n\ndnr.2018 %>% \n  select(any_of(colnames(dnr.2021)))\n\n# A tibble: 539 × 3\n   collected_date      environmental_location   p_h\n   <dttm>              <chr>                  <dbl>\n 1 2018-05-22 11:19:00 Backbone Beach           8.1\n 2 2018-06-19 09:00:00 Backbone Beach           8.8\n 3 2018-08-21 13:00:00 Backbone Beach           8.9\n 4 2018-07-10 11:15:00 Backbone Beach           8.2\n 5 2018-08-28 12:30:00 Backbone Beach           8.8\n 6 2018-08-14 10:30:00 Backbone Beach           8  \n 7 2018-08-07 11:10:00 Backbone Beach           7.7\n 8 2018-07-24 12:00:00 Backbone Beach           8.9\n 9 2018-07-31 11:25:00 Backbone Beach           8.5\n10 2018-07-02 11:15:00 Backbone Beach           7.9\n# … with 529 more rows\n\n\nWe’re missing a lot of columns here. Let’s do another rename step here before our select step:\n\ndnr.2018 <- dnr.2018 %>% \n  rename(\n    microcystin = microcystin_raw_value_ug_l,\n    tkp = tkp_mg_p_l,\n    tkn = tkn_mg_n_l,\n    x16s = x16s_r_rna_gene_copies_m_l,\n    mcy_a_m = microcystismcy_a_gene_copies_m_l,\n    mcy_a_p = planktothrixmcy_a_gene_copies_m_l,\n    mcy_a_a = aanabaenamcy_a_gene_copies_m_l\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n\ndnr.2018 %>% \n  head()\n\n# A tibble: 6 × 10\n  collected_date      environmental_locati… microcystin   tkp   tkn   p_h   x16s\n  <dttm>              <chr>                       <dbl> <dbl> <dbl> <dbl>  <dbl>\n1 2018-05-22 11:19:00 Backbone Beach              0.63   1.61 0.66    8.1 1.47e5\n2 2018-06-19 09:00:00 Backbone Beach              0.46   1.06 0.517   8.8 4.23e2\n3 2018-08-21 13:00:00 Backbone Beach              0.438  1.06 0.169   8.9 4.04e7\n4 2018-07-10 11:15:00 Backbone Beach              0.425  1.22 0.505   8.2 1.08e7\n5 2018-08-28 12:30:00 Backbone Beach              0.418  1.02 1.14    8.8 7.65e6\n6 2018-08-14 10:30:00 Backbone Beach              0.405  1.15 0.298   8   4.46e7\n# … with 3 more variables: mcy_a_m <dbl>, mcy_a_p <dbl>, mcy_a_a <dbl>\n\n\nMuch better. Note that there are some missing columns here because there are columns in the 2021 dataset that don’t exist in the 2018 dataset.\n\n\n2019 Data\nWe’ll repeat this process for the 2019 dataset. Some notes:\n\nclean_names already nicely formatted the mcyX columns\nBy default, read_xlsx reads in the first sheet of the xlsx file. This means that the 2019 data will only show the data on the first sheet. We want the data from the sheet labeled “combined”, so use the sheet argument of read_xlsx to do this.\n\n\ndnr.2019 <- read_xlsx(\"data/IowaDNR_2019_Data_Merged.xlsx\",\n                      sheet = \"combined\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date)) %>% \n  rename(\n    tkn = tkn_mg_n_l,\n    tkp = tkp_mg_p_l,\n    ortho_p = ortho_p_mg_p_l\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n\n\n\n\n\ndnr.2019 %>% \n  head()\n\n# A tibble: 6 × 12\n  label    collected_date      environmental_l… microcystin    tkp ortho_p   tkn\n  <chr>    <dttm>              <chr>                  <dbl>  <dbl>   <dbl> <dbl>\n1 6-21280… 2019-06-25 00:00:00 Backbone Beach         0      0.473  0.065  0.18 \n2 7-21280… 2019-07-01 00:00:00 Backbone Beach         0      0.023  0.034  0.282\n3 8-21280… 2019-07-09 00:00:00 Backbone Beach         0.23   0.056  0.0401 0.346\n4 9-21280… 2019-07-16 00:00:00 Backbone Beach         0.743 -0.005  0.023  0.365\n5 10-2128… 2019-07-23 00:00:00 Backbone Beach         0.038  0.287  0.081  0.858\n6 11-2128… 2019-07-30 00:00:00 Backbone Beach         0.105  0.199  0.22   1.18 \n# … with 5 more variables: p_h <dbl>, x16s <dbl>, mcy_a_m <dbl>, mcy_a_p <dbl>,\n#   mcy_a_a <dbl>\n\n\n\n\n2020 Data\nPretty much the same process as above, though we need to cast the ortho_p, tkn, and tkp columns as a numeric to account for places where the text value “NA” is entered:\n\ndnr.2020 <- read_xlsx(\"data/IowaDNR_2020_Data_Merged.xlsx\") %>% \n  clean_names() %>% \n  filter(!is.na(collected_date)) %>% \n  rename(\n    tkp = tkp_mg_p_l,\n    tkn = tkn_mg_n_l, \n    ortho_p = ortho_p_mg_p_l\n  ) %>% \n  mutate(\n    across(c(ortho_p, tkn, tkp), ~ as.numeric(.))\n  ) %>% \n  select(any_of(colnames(dnr.2021)))\n\n\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\n\n\nPutting it all together\nSince we did all of the cleaning ahead of time, combining the columns now will be straightforward using bind_rows. Let’s save this to a new variable called dnr_all:\n\ndnr.all <- bind_rows(\n  dnr.2018,\n  dnr.2019,\n  dnr.2020,\n  dnr.2021\n)\n\n\n\n\n\n\n\nShould we have written functions?\n\n\n\n\n\nNotice that there were a lot of redundant steps across all of the DNR sets. For each data set, we:\n\nRead in the Excel spreadsheet\nCalled clean_names\nFiltered na dates\nRenamed some columns\nSelected the columns that match the names of the 2021 dataset.\n\nIdeally, we would have written a function to handle all of the redundant tasks. For example, we could have written a function read_dnr_sheet that takes in a file path, reads in the Excel sheet, calls clean_names, filters na dates, renames columns, and selects only those columns that match those from the 2021 data.\nSo yes, it would have been nice to write functions. However, the problem is that the data sheets are all just different enough that this isn’t exactly quite worth the effort. For instance, the 2019 data requires the sheet argument, which complicates the potential function definition. Similarly, the column names that are required to be renamed don’t quite match up between the different years, and so would require an additional argument."
  },
  {
    "objectID": "docs/meetings/first_model.html#how-many-hazardous-cases-were-there-per-year",
    "href": "docs/meetings/first_model.html#how-many-hazardous-cases-were-there-per-year",
    "title": "Beginning Predictions",
    "section": "How many hazardous cases were there per year?",
    "text": "How many hazardous cases were there per year?\n\ntheme_set(theme_light())\n\ndnr.all %>% \n  filter(hazard_class == \"hazardous\") %>% \n  count(year) %>% \n  ggplot(aes(year, n, fill = n)) +\n  geom_col() + \n  scale_fill_viridis_c(option = \"inferno\", direction = -1, end = 0.5) + \n  labs(\n    x = \"Sample year\",\n    y = \"# HABs Observed\",\n    title = \"# Harmful algal blooms observed per year\",\n    fill = \"# HABs\\nobserved\"\n  ) + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.border = element_blank(),\n    axis.line.x = element_line(color = \"black\", size = 0.5),\n    axis.line.y = element_line(color = \"black\", size = 0.5),\n    axis.ticks = element_blank()\n  ) + \n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1), add = 0)\n  )"
  },
  {
    "objectID": "docs/meetings/first_model.html#which-locations-had-the-most-harmful-algal-blooms",
    "href": "docs/meetings/first_model.html#which-locations-had-the-most-harmful-algal-blooms",
    "title": "Beginning Predictions",
    "section": "Which locations had the most harmful algal blooms?",
    "text": "Which locations had the most harmful algal blooms?\n\ndnr.all %>% \n  filter(hazard_class == \"hazardous\") %>% \n  count(environmental_location) %>%\n  mutate(environmental_location = fct_reorder(environmental_location, n)) %>% \n  ggplot(aes(n, environmental_location, fill = n)) + \n  geom_col() + \n  labs(\n    x = \"# HABS observed (all years)\",\n    y = \"Sample Location\",\n    title = \"Total number of HABs observed for all years\",\n    fill = \"# HABs\\nObserved\"\n  ) + \n  scale_fill_viridis_c(option = \"magma\", direction = -1) + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.ticks = element_blank()\n  ) + \n  scale_x_continuous(\n    expand = expansion(mult = c(0, 0.1), add = 0)\n  )"
  },
  {
    "objectID": "docs/meetings/first_model.html#splitting-the-data-preprocessing",
    "href": "docs/meetings/first_model.html#splitting-the-data-preprocessing",
    "title": "Beginning Predictions",
    "section": "Splitting the data, preprocessing",
    "text": "Splitting the data, preprocessing\nWe begin by splitting our data into testing and training splits:\n\nlibrary(tidymodels)\nset.seed(489)\n\ndata_split <- initial_split(dnr.all, strata = \"hazard_class\")\n\ntraining_data <- training(data_split)\ntesting_data <- testing(data_split)\n\nNext, we create a recipe. A recipe is a series of data processing steps to perform on our data before we feed it into our model. Here, the only steps we’ll take is to remove non-informative variables and to impute some missing values by filling in the mean.\n\nhab_recipe <- recipe(microcystin ~ ., training_data) %>% \n  step_rm(collected_date, label) %>% \n  step_impute_mean(all_numeric())\n\nWe can take a look at the preprocessed data set by using the prep and juice functions. prep will perform the steps outlined in the pre-processing recipe. For example, in our recipe above, it will calculate the mean of all the numeric columns. juice will then return the transformed dataset resulting from following all the steps in the recipe:\n\nhab_recipe %>% \n  prep %>% \n  juice\n\n# A tibble: 1,193 × 13\n   environmental_loca…   tkp    tkn   p_h   x16s mcy_a_m mcy_a_p mcy_a_a ortho_p\n   <fct>               <dbl>  <dbl> <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Backbone Beach      1.61   0.66   8.1  1.47e5       0       0       0  0.0430\n 2 Backbone Beach      1.06   0.517  8.8  4.23e2       0       0       0  0.0430\n 3 Backbone Beach      1.22   0.505  8.2  1.08e7    1240       0       0  0.0430\n 4 Backbone Beach      1.02   1.14   8.8  7.65e6       0       0       0  0.0430\n 5 Backbone Beach      1.15   0.298  8    4.46e7       0       0       0  0.0430\n 6 Backbone Beach      0.922 -0.24   7.7  2.66e7       0       0       0  0.0430\n 7 Backbone Beach      0.041  0.233  8.9  5.04e6       0       0       0  0.0430\n 8 Backbone Beach      0.491  0.264  8.5  1.09e7     432       0       0  0.0430\n 9 Backbone Beach      1.12   0.437  7.9  6.09e6       0       0       0  0.0430\n10 Backbone Beach      1.22   0.569  7.28 4.44e6       0       0       0  0.0430\n# … with 1,183 more rows, and 4 more variables: dissolved_oxygen_mg_l <dbl>,\n#   hazard_class <fct>, year <dbl>, microcystin <dbl>\n\n\nWe see that the collected_date and label were removed, and the numeric columns had the missing data filled in with the column mean."
  },
  {
    "objectID": "docs/meetings/first_model.html#creating-and-training-a-model",
    "href": "docs/meetings/first_model.html#creating-and-training-a-model",
    "title": "Beginning Predictions",
    "section": "Creating and training a model",
    "text": "Creating and training a model\nNow that we have our data preprocessing steps, we’ll create the model that we want to use to predict. To begin with, we’ll just use an lm model. We’ll indicate this to tidymodels using the linear_reg function.\n\nlinear_model <- linear_reg() %>% \n  set_engine(\"lm\") %>% \n  set_mode(\"regression\")\nlinear_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThis tells us that linear_model is a tidymodels object using the lm function to perform linear regression.\nNow we can package the preprocessing recipe and the model into a workflow. Think of the workflow as a pipeline - a series of steps you want to carry out on the data.\n\nlm_workflow <- workflow() %>% \n  add_recipe(hab_recipe) %>% \n  add_model(linear_model) \nlm_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_impute_mean()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThis shows us all of the preprocessing steps we’re going to carry out on the data as well as the model we’re going to use. We’re now ready to fit our model to the training data:\n\nlm_fit <- lm_workflow %>% \n  last_fit(split = data_split)\n\n! train/test split: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici...\n\nlm_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [1193/398]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n\nUse `collect_notes(object)` for more information.\n\n\nThe last_fit function takes the entire data split, performs the preprocessing and model training steps on the training set, then predicts on the testing set. There are several convenience functions we can use to interact with the lm_fit object. To start, we can collect the metrics via collect_metrics:\n\nlm_fit %>% \n  collect_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       3.42  Preprocessor1_Model1\n2 rsq     standard       0.692 Preprocessor1_Model1\n\n\nWe can also collect the predictions via collect_predictions:\n\nlm_fit %>% \n  collect_predictions() \n\n# A tibble: 398 × 5\n   id               .pred  .row microcystin .config             \n   <chr>            <dbl> <int>       <dbl> <chr>               \n 1 train/test split 0.967     3       0.438 Preprocessor1_Model1\n 2 train/test split 0.661    13       0.153 Preprocessor1_Model1\n 3 train/test split 1.53     21       0.377 Preprocessor1_Model1\n 4 train/test split 2.12     23       0.195 Preprocessor1_Model1\n 5 train/test split 2.03     30       0     Preprocessor1_Model1\n 6 train/test split 0.654    31       0.855 Preprocessor1_Model1\n 7 train/test split 0.837    34       0.69  Preprocessor1_Model1\n 8 train/test split 0.677    36       0.405 Preprocessor1_Model1\n 9 train/test split 0.673    43       0.003 Preprocessor1_Model1\n10 train/test split 0.423    44       0     Preprocessor1_Model1\n# … with 388 more rows\n\n\nThis is perhaps more useful when visualized:\n\nlm_fit %>% \n  collect_predictions() %>% \n  ggplot(aes(.pred, microcystin)) + \n  geom_point(color = \"#0000FF\", alpha = 0.5) + \n  geom_abline(color = \"red\", linetype = 2, alpha = 0.5) + \n  theme(\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Linear Regression Prediction Results - dnr.all dataset\"\n  ) \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nTypically when working with tidymodels, there are a few intermediate steps before jumping straight to last_fit and looking at predictions. For another illustration of using tidymodels on the built-in diamonds data set, see the code below, borrowed from this StackOverflow post:\n\nset.seed(489)\ndiamonds_split <- initial_split(diamonds, prop = 4/5)\n\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n\ndiamonds_recipe <- \n  recipe(price ~ ., data = diamonds_train) %>%\n  step_log(all_outcomes(),skip = T) %>%\n  step_normalize(all_predictors(), -all_nominal()) %>%\n  step_dummy(all_nominal()) %>%\n  step_poly(carat, degree = 2)\n\n\nlr_model <- \n  linear_reg()%>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nlr_workflow <- workflow() %>%\n  add_model(lr_model) %>% \n  add_recipe(diamonds_recipe) \n\nfinal_model <- fit(lr_workflow, diamonds)\n\npredict(final_model, new_data = as.data.frame(diamonds_test))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GERMS-DS",
    "section": "",
    "text": "Schedule\nThe first GERMS-DS meeting is on Monday, April 11 at 9AM in Elings 4321. Future meetings will be held every other Monday at the same time.\n\n\nWhat you need to get started\n\nInstall R and the latest version of RStudio\nInstall Quarto\nInstall the tidyverse, tidymodels, rmarkdown, janitor, lubridate, and readxl libraries"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]